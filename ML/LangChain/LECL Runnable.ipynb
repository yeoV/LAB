{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LECL Basic Code\n",
    "\n",
    "- Using  Ollama model\n",
    "\n",
    "ref : https://python.langchain.com/docs/expression_language/why/#lcel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_community.chat_models import ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompt 생성\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOllama()"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser = StrOutputParser()\n",
    "model = ChatOllama() # llama2\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhy did the F1 driver break up with his girlfriend?\\n\\nShe was always turning left!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = (\n",
    "    {\"topic\" : RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"F1 drivers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runnable\n",
    "- RunnablePassthrough\n",
    "- RunnableParallel\n",
    "- RunnableLambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableParallel, RunnableLambda\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"{country} 의 수도는 어디야 ?\")\n",
    "model = ChatOllama() # llama2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n대한민국의 수도는  Séoul (서울)이다.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnablePassthrough\n",
    "chain = (\n",
    "    {\"country\" : RunnablePassthrough() }\n",
    "    | prompt \n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain.invoke(\"대한민국\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'수도': '\\nThe capital of South Korea is Seoul.',\n",
       " '인구': '\\nAs of 2023, the estimated population of South Korea is around 51.7 million people.'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnableParallel\n",
    "\n",
    "prompt1 = ChatPromptTemplate.from_template(\"{country} 의 수도는 어디야 ?\")\n",
    "prompt2 = ChatPromptTemplate.from_template(\"{country} 의 인구는 몇명이야 ?\")\n",
    "\n",
    "chain1 = (\n",
    "    {\"country\" : RunnablePassthrough() }\n",
    "    | prompt1\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "chain2 = (\n",
    "    {\"country\" : RunnablePassthrough() }\n",
    "    | prompt2\n",
    "    | model\n",
    "    | output_parser\n",
    ")\n",
    "\n",
    "map_chain = RunnableParallel(수도=chain1, 인구=chain2)\n",
    "\n",
    "\n",
    "chain.invoke(\"대한민국\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe capital of South Korea is Seoul. \\nAs of December 2022, the estimated population of South Korea is around 51.7 million people.'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RunnableLambda\n",
    "\n",
    "def combine_text(text):\n",
    "    return text[\"수도\"] + \" \" + text[\"인구\"]\n",
    "\n",
    "lambda_chain = (\n",
    "    map_chain\n",
    "    | RunnableLambda(combine_text)\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "lambda_chain.invoke(\"대한민국\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n서울이 대한민국의 수도다.\\n국립통계위원회(Kostat)에 따라 最新의 인구조사를 통해, 2022년 대한민국의 인구가 약 5180만명으로 Estimated to be.\\n\\n🇰🇷🏙️'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RAG 의 경우\n",
    "rag_chain = (\n",
    "    map_chain\n",
    "    | {\"info\" : RunnableLambda(combine_text)}\n",
    "    | ChatPromptTemplate.from_template(\"다음의 내용을 한국어로 번역하고, 적절한 곳에 적절한 이모티콘을 추가해줘 : {info}\")\n",
    "    | model\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"대한민국\").content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
